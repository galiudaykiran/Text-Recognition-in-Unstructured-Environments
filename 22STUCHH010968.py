# -*- coding: utf-8 -*-
"""Cv.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xw_yXRCj5hSNgF7aqtNEA5kZX4yoU-6o
"""

# Install required packages
!pip install easyocr
!pip install opencv-python
!pip install numpy
!pip install matplotlib
!pip install imutils
!pip install Pillow

# Import the necessary libraries
import os
import cv2
import time
import numpy as np
import matplotlib.pyplot as plt
from google.colab.patches import cv2_imshow
from PIL import Image, ImageDraw, ImageFont

# For uploading images in Google Colab
from google.colab import files
import io

# Helper functions for visualization
def plot_results(image, boxes, texts, scores=None, title="Text Detection Results"):
    """
    Visualize text detection results on an image
    """
    # Convert image to RGB if it's in BGR
    if len(image.shape) == 3 and image.shape[2] == 3:
        if isinstance(image, np.ndarray):
            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        else:
            image_rgb = image
    else:
        image_rgb = image

    plt.figure(figsize=(10, 10))
    plt.imshow(image_rgb)

    # Plot bounding boxes and text
    for i, (box, text) in enumerate(zip(boxes, texts)):
        # Convert box format if needed
        if len(box) == 4:  # [x1, y1, x2, y2] format
            x1, y1, x2, y2 = box
            rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, color='red', linewidth=2)
            plt.gca().add_patch(rect)
            plt.text(x1, y1-5, f"{text}", color='red', fontsize=12,
                     bbox=dict(facecolor='white', alpha=0.8))
        else:  # polygon format [(x1,y1), (x2,y2), ...]
            box_np = np.array(box)
            plt.plot(box_np[:, 0], box_np[:, 1], 'r', linewidth=2)
            plt.text(box_np[0, 0], box_np[0, 1]-5, f"{text}", color='red', fontsize=12,
                     bbox=dict(facecolor='white', alpha=0.8))

    plt.title(title)
    plt.axis('off')
    plt.tight_layout()
    plt.show()

def draw_text_detection(image, results, box_color=(0, 255, 0), text_color=(255, 0, 0), font_scale=0.6):
    """
    Draw text detection results on an image
    """
    img_copy = image.copy()

    # Loop through the results
    for (bbox, text, prob) in results:
        # Get bounding box coordinates
        points = np.array(bbox).astype(np.int32).reshape((-1, 1, 2))

        # Draw the bounding box
        cv2.polylines(img_copy, [points], True, box_color, 2)

        # Calculate coordinates for the text
        x = int(bbox[0][0])
        y = int(bbox[0][1]) - 10

        # Draw the text
        cv2.putText(img_copy, text, (x, y), cv2.FONT_HERSHEY_SIMPLEX, font_scale, text_color, 2)

    return img_copy

# EasyOCR Implementation
def text_recognition_easyocr(image_path, lang_list=['en'], gpu=False, verbose=True):
    """
    Detect and recognize text using EasyOCR
    """
    import easyocr

    # Create reader with the desired languages
    print("Initializing EasyOCR Reader...")
    reader = easyocr.Reader(lang_list, gpu=gpu, verbose=verbose)

    # Read image
    if isinstance(image_path, str):
        image = cv2.imread(image_path)
    else:
        image = image_path

    # Detect text
    print("Detecting text...")
    start_time = time.time()
    results = reader.readtext(image)
    end_time = time.time()

    print(f"Text detection completed in {end_time - start_time:.2f} seconds")
    print(f"Found {len(results)} text regions")

    # Process results
    boxes = [result[0] for result in results]
    texts = [result[1] for result in results]
    scores = [result[2] for result in results]

    # Draw results on the image
    annotated_image = draw_text_detection(image, results)

    return results, boxes, texts, scores, annotated_image

# Image Preprocessing Functions
def preprocess_image(image, method='default'):
    """
    Preprocess image for better OCR results
    """
    # Load the image if path is provided
    if isinstance(image, str):
        image = cv2.imread(image)

    # Convert to grayscale
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    if method == 'default':
        # Basic preprocessing: Gaussian blur + Adaptive threshold
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        processed = cv2.adaptiveThreshold(blurred, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                         cv2.THRESH_BINARY, 11, 2)

    elif method == 'threshold':
        # Global thresholding
        _, processed = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)

    elif method == 'adaptive':
        # Adaptive thresholding
        processed = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                         cv2.THRESH_BINARY, 11, 2)

    elif method == 'otsu':
        # Otsu's thresholding
        _, processed = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

    elif method == 'canny':
        # Edge detection with Canny
        processed = cv2.Canny(gray, 100, 200)

    elif method == 'sharpen':
        # Sharpening
        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
        processed = cv2.filter2D(gray, -1, kernel)

    else:
        # Default to grayscale if method not recognized
        processed = gray

    return processed

# Text Analysis Functions
def analyze_text_results(texts, boxes, scores=None):
    """
    Analyze recognized text and provide insights
    """
    print("\n===== Text Analysis Results =====")
    print(f"Total text regions detected: {len(texts)}")

    # Count characters, words, and numbers
    total_chars = sum(len(text) for text in texts)
    total_words = sum(len(text.split()) for text in texts)

    # Count numeric regions
    numeric_regions = sum(1 for text in texts if text.strip().isdigit())

    print(f"Total characters: {total_chars}")
    print(f"Total words: {total_words}")
    print(f"Numeric regions: {numeric_regions}")

    # Display text content
    print("\nDetected Text Content:")
    for i, text in enumerate(texts):
        confidence = f" (Confidence: {scores[i]:.2f})" if scores is not None and i < len(scores) else ""
        print(f"Region {i+1}: '{text}'{confidence}")

    # If more than one text region, attempt to combine them
    if len(texts) > 1:
        combined_text = " ".join(texts)
        print("\nCombined Text:")
        print(combined_text)

    return {
        'total_regions': len(texts),
        'total_chars': total_chars,
        'total_words': total_words,
        'numeric_regions': numeric_regions,
        'texts': texts,
        'combined_text': " ".join(texts) if len(texts) > 1 else texts[0] if texts else ""
    }

# Text Analysis Functions
def analyze_text_results(texts, boxes, scores=None):
    """
    Analyze recognized text and provide insights
    """
    print("\n===== Text Analysis Results =====")
    print(f"Total text regions detected: {len(texts)}")

    # Count characters, words, and numbers
    total_chars = sum(len(text) for text in texts)
    total_words = sum(len(text.split()) for text in texts)

    # Count numeric regions
    numeric_regions = sum(1 for text in texts if text.strip().isdigit())

    print(f"Total characters: {total_chars}")
    print(f"Total words: {total_words}")
    print(f"Numeric regions: {numeric_regions}")

    # Display text content
    print("\nDetected Text Content:")
    for i, text in enumerate(texts):
        confidence = f" (Confidence: {scores[i]:.2f})" if scores is not None and i < len(scores) else ""
        print(f"Region {i+1}: '{text}'{confidence}")

    # If more than one text region, attempt to combine them
    if len(texts) > 1:
        combined_text = " ".join(texts)
        print("\nCombined Text:")
        print(combined_text)

    return {
        'total_regions': len(texts),
        'total_chars': total_chars,
        'total_words': total_words,
        'numeric_regions': numeric_regions,
        'texts': texts,
        'combined_text': " ".join(texts) if len(texts) > 1 else texts[0] if texts else ""
    }

# Install dependencies
!pip install easyocr opencv-python-headless matplotlib

import cv2
import numpy as np
import easyocr
import matplotlib.pyplot as plt
from google.colab import files

# Function to preprocess image (adaptive thresholding)
def preprocess_image(image, method='adaptive'):
    """
    Preprocess the image to enhance text visibility.
    Args:
        image: Input image (BGR).
        method: Preprocessing method ('adaptive' for adaptive thresholding).
    Returns:
        Processed grayscale image.
    """
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    if method == 'adaptive':
        processed = cv2.adaptiveThreshold(
            gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2
        )
    else:
        processed = gray
    return processed

# Function to perform text recognition with EasyOCR
def text_recognition_easyocr(image):
    """
    Perform text detection and recognition using EasyOCR.
    Args:
        image: Numpy array (BGR).
    Returns:
        results: Raw EasyOCR results.
        boxes: Bounding boxes for detected text.
        texts: Extracted text strings.
        scores: Confidence scores.
        annotated_image: Image with bounding boxes drawn.
    """
    reader = easyocr.Reader(['en'], gpu=True)  # Initialize EasyOCR for English
    results = reader.readtext(image)

    boxes = []
    texts = []
    scores = []
    annotated_image = image.copy()

    for (bbox, text, prob) in results:
        (top_left, top_right, bottom_right, bottom_left) = bbox
        top_left = (int(top_left[0]), int(top_left[1]))
        bottom_right = (int(bottom_right[0]), int(bottom_right[1]))
        boxes.append([top_left, bottom_right])
        texts.append(text)
        scores.append(prob)

        # Draw bounding box and text
        cv2.rectangle(annotated_image, top_left, bottom_right, (0, 255, 0), 2)
        cv2.putText(annotated_image, text, (top_left[0], top_left[1] - 10),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 2)

    return results, boxes, texts, scores, annotated_image

# Function to analyze text results
def analyze_text_results(texts, boxes, scores):
    """
    Analyze OCR results to compute metrics.
    Args:
        texts: List of detected text strings.
        boxes: List of bounding boxes.
        scores: List of confidence scores.
    Returns:
        Dictionary with analysis metrics.
    """
    total_regions = len(texts)
    total_chars = sum(len(text) for text in texts)
    total_words = sum(len(text.split()) for text in texts)

    return {
        'total_regions': total_regions,
        'total_chars': total_chars,
        'total_words': total_words,
        'average_confidence': np.mean(scores) if scores else 0.0
    }

# Main function for scene text recognition
def scene_text_recognition_example():
    """
    Complete example of scene text recognition with uploaded image
    """
    print("Scene Text Recognition in Unstructured Environments")
    print("==================================================")

    # Step 1: Upload image
    print("Step 1: Please upload an image containing text (e.g., JPG or PNG).")
    print("Click 'Choose Files' below and select your image.")
    uploaded = files.upload()

    if not uploaded:
        print("Error: No image uploaded.")
        raise FileNotFoundError("No image file uploaded.")

    image_file = list(uploaded.keys())[0]
    image = cv2.imdecode(np.frombuffer(uploaded[image_file], np.uint8), cv2.IMREAD_COLOR)

    if image is None:
        print("Error: Could not load the image. Ensure it's a valid image file (e.g., JPG, PNG).")
        raise ValueError("Invalid image file.")

    print(f"Processing image: {image_file}")

    # Display the original image
    plt.figure(figsize=(10, 8))
    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    plt.title("Original Image")
    plt.axis('off')
    plt.show()

    # Step 2: Apply OCR on original image
    print("\nStep 2: Performing text detection on original image...")
    results, boxes, texts, scores, annotated_image = text_recognition_easyocr(image)

    # Display the detection results
    plt.figure(figsize=(10, 8))
    plt.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))
    plt.title("Text Detection Results on Original Image")
    plt.axis('off')
    plt.show()

    # Analyze the original text results
    analysis_original = analyze_text_results(texts, boxes, scores)

    # Step 3: Preprocess the image for better OCR results
    print("\nStep 3: Preprocessing image for better OCR results...")
    processed_image = preprocess_image(image, method='adaptive')

    # Display the preprocessed image
    plt.figure(figsize=(10, 8))
    plt.imshow(processed_image, cmap='gray')
    plt.title("Preprocessed Image (Adaptive Thresholding)")
    plt.axis('off')
    plt.show()

    # Convert back to BGR for EasyOCR
    processed_image_bgr = cv2.cvtColor(processed_image, cv2.COLOR_GRAY2BGR)

    # Step 4: Apply OCR on preprocessed image
    print("\nStep 4: Performing text detection on preprocessed image...")
    results_processed, boxes_processed, texts_processed, scores_processed, annotated_image_processed = text_recognition_easyocr(processed_image_bgr)

    # Display the detection results for preprocessed image
    plt.figure(figsize=(10, 8))
    plt.imshow(cv2.cvtColor(annotated_image_processed, cv2.COLOR_BGR2RGB))
    plt.title("Text Detection Results on Preprocessed Image")
    plt.axis('off')
    plt.show()

    # Analyze the preprocessed text results
    analysis_processed = analyze_text_results(texts_processed, boxes_processed, scores_processed)

    # Step 5: Compare results
    print("\nStep 5: Comparing OCR results between original and preprocessed images")
    char_diff = analysis_processed['total_chars'] - analysis_original['total_chars']
    word_diff = analysis_processed['total_words'] - analysis_original['total_words']
    region_diff = analysis_processed['total_regions'] - analysis_original['total_regions']

    print(f"\nImprovement Summary:")
    print(f"Change in detected regions: {region_diff:+d}")
    print(f"Change in characters: {char_diff:+d}")
    print(f"Change in words: {word_diff:+d}")
    print(f"Average confidence (original): {analysis_original['average_confidence']:.2f}")
    print(f"Average confidence (preprocessed): {analysis_processed['average_confidence']:.2f}")
    print("\nDetected Texts (Original Image):")
    for i, text in enumerate(texts, 1):
        print(f"{i}. {text}")

    print("\nDetected Texts (Preprocessed Image):")
    for i, text in enumerate(texts_processed, 1):
        print(f"{i}. {text}")

    print("\nExample completed! The scene text recognition system has successfully detected and extracted text from the uploaded image.")

# Run the main example function
scene_text_recognition_example()